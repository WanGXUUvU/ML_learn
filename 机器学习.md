## 线性模型
分类：输出一个m维向量，第i个表示模型属于第i个的概率是多少，也叫置信度
类oi的置信度 oi=<x,wi>+bi
one-hot-Encoding：只有1个是1其余都是0
Minimize MSE 
预测结果就是最大的oi所属的那一类
缺点：关注太不正确的类，导致模型学习的样本太多。比如有1w个类，而我们只关心它属于哪个类别
优化：把o经过一个softmax层，yhat=这个值，还是线形因为我们只关心最大值，他是递增的。
而y=[y1,y2...yn] ，只有目标类别 yi才会为1                                                                                                                             
把y和yhat做交叉熵就可以忽略掉其他yi为0的类别

## Mini-batch随机梯度下降
## RNN
![](assets/17565292624250.png)

H包含了过去所有状态的信息![](assets/17565294071321.png)

Whh和Whx本质是一个W 可以理解为在不同时间维度的展开
### Bi-RNN 
双向 看过去也看未来
### Deep RNN
## 模型评估
**loss value**被广泛的用来判断一个模型的好坏

展示广告 二分类
CTR：点击率*price 预期收入

- Accuracy:正确预测/总数
- Precison:预测成了类i而且真的是类i/预测为类i的个数（预测的准确率）
- Recall：预测成了类i而且真的是类i/类i的个数
- F1：平衡Precsion和Recall

AUC 曲线的面积
![](assets/17566125634599.png)
AUC=1 效果最好
0.5特别糟糕
### 几个其他指标
- Latency：广告应该被在一个时间内展示出来
- ASN：平均每页展示多少广告
- CTR：真实用户点击率
- ACP：平均价格每次点击
- revenue：流量* ASN * CTR * ACP 
## 过拟合和欠拟合
**训练误差**：在训练集上的误差
**泛化误差**：在新的数据集上的误差
### 模型复杂度影响
增加模型复杂度，训练误差会降低，但是泛化误差不一定下降
选择有一点过拟合的模型效果最好
### 数据复杂度
网络越复杂，用更大的数据集泛化误差越低
总之数据越简单选择的模型也越简单，越复杂，选择的模型也越复杂
## 模型验证
test dataset：只能用一次
Validation dataset：验证数据集可以使用多次，通常是测试数据集中的一块
### 生成验证集的方法
把数据集分为train和valid
### k折交叉验证
![](assets/17567207150166.png)

取平均的泛化误差，数据大的时候选择k小一点，反之大一点
### 常见错误
- 验证集的里面有训练集的错误
- 信息泄露：股票预测可能训练集已经包含了验证集的信息

## 方差和偏差
总误差 = 偏差² + 方差 + 噪声
Error = Bias² + Variance + Noise
![](assets/17567212692925.png)
模型复杂度 ↗️

简单模型 ────────────────→ 复杂模型
高偏差                      高方差
低方差                      低偏差
欠拟合                      过拟合
    ↘️        最优点        ↙️
         总误差最小
## bagging
降低方差
随机森林效果最佳
训练n个模型，独立并行训练
预测：n个模型取平均
分类：n个模型投票
每个模型通过bootstrap采样，随机有放回的采样，同一个样本可以被拿多次大概是63%的样本，剩下的out of bag
## Boosting
降低偏差
不断对一个模型进行训练每次关心那些训练不好的样本![](assets/17567242235728.png)
## stacking
![](assets/17568042560788.png)
对于同一个数据集用不同的模型，经过一个全连接层。加权投票。降低方差
### 多层stacking
![](assets/17568045784408.png)

容易过拟合：解决 把训练集分为A B 第一层在A上训练，然后对B进行预测，把预测结果和B的本身作为第二层的输入
### k折交差验证stacking
每一层分成k份，k个模型对对应的那块数据都有一次没有训练的部分，对这块数据预测，然后并起来，可以得到输入相同类型的，把它作为下层的输入
进一步降低过拟合：把n层的预测平均一下再输入到下一层

## 模型调参
SGD如果调好的话效果比Adam效果好一点
Adam作用范围比较宽泛
超参数 误差记录在Excel中
tenesorboard weights & bias
### 自动调参
## 超参数优化
- Multi-fidelity：因为训练一个模型太贵了（数据集很大，完整跑完很耗时间，还要试很多的话，太耗时了），所以可以不用把整个数据集给跑完（不关心最后的精度怎么样，只关心超参数之间的效果怎么样）；以下是做法：
	- SSH 最靠谱的超参数训练完，每次选一半的结果好的超参n，训练时间m加倍，最后成本是n*m，最终是一个固定值
	- Hyperband 跑多个SSH，调整n和m但是n*m不变
- 黑盒，调参真的训练出一个模型
	- 网格搜索：所有组合都试一次，保证找到最好
	- 随机搜索：最多试n次每次每个超参随机选一个
	- 贝叶斯优化：根据已经采样的点拟合下一个要采的超参数点。
- Black-box 虽然会贵一点但是任务计算量比较小或优化算法不知道的话，这个方法会比较好；Multi-fidelity知道一些任务的细节，可以将任务弄小一点，这样每次试验的时候成本没有那么高。

## 网络架构搜索 NAS
自动构建一个最优的神经网络

## 深度神经网络
### 归一化
在线性模型中，我们会对数据进行标准化（使得数据的每一个特征均值为0方差为1），从数学上来说，这样能使得损失函数更加平滑（特别是对于线性模型来说）
平滑是指损失函数对x的导数与损失对y的导数 间的差的平方和，会小于等于x与y之间的差的平方和的β倍。这意味的是说x走的很远的时候，它的梯度变化得不是很大 （我走了很远但是方向没有走得太偏）
![](assets/17568112361774.png)
因此，β比较小得时候，可以有一个比较大得学习率（学习率等价于步长）；
但是标准化不会帮助深度神经网络，因为如果是对x做标准化的话，他只会帮助直接线性作用于x上的那个函数，也就是线性模型是可以的。但在多层的情况下，他会帮助最下面那层的线性层；
## BN
批量归一化就是说，把中间那些层的输入也做标准化，这样能帮助整个函数更加平滑，使得在训练深度神经网络的时候会更加容易（这个观点还是争议的）；
使用批量归一化之后，收敛上会更加容易，可以选用更大的一些学习率，但是一般来说不会改变最后的结果（精度跟没有差不多，但是可以快一点）
![](assets/17568113941835.jpg)
- 变形（Reshape）：如果输入是2维的矩阵就不用改变，不是的话就要改成2维的【举个例子，假设输入是个卷积（一般是四维的，n（批量大小维）、c（RGB 通道或卷积的输出通道）、w（宽）、h（高）），我们会在这一步将其变化为2维的矩阵，由n * c * w * h变为nwh * c ，就是把通道维拉到最后，把nwh这三个维度合并在一起，可以这么理解 c 在CNN中表示的是一个识别出特征，而nwh则是样本的数据】
- 标准化（Normalization）：具体来说是对每一列标准化，也就是对变形后的矩阵的一列 减去 这一列的均值 再除以这一列的方差，；
- 还原（Recover）：用我们标准化后的矩阵y 对它的一列乘上 这一列对应的γ 加上这一列对应的β，在这个地方是说虽然我们对数据减了均值后再除了方差，但是我们还是有点想要数据有一点偏差，那么这个步骤就允许这个还原回去【如果γ为方差，β为均值，那么将会还原回去】，在这里γ与β是可以学习的， 神经网络会根据需求去找谁会更好一点；
## LN
层归一化主要是用在循环神经网络（RNN）里面，因为将BN用于RNN中时每一个时间步骤都得用自己得均值方差，甚至是学到得γ与β在每一时间步中最好不要共享这些均值与方差（在不同的时间步中这些数值变化还是很大的，而BN需要一个比较稳定的均值方差的估计，抖动比较大的均值与方差就失去了做标准化的意义）
层归一化到底是在做什么：在变形的步骤上对输入矩阵做转置（2维就普通转置4维就把cwh放在一起再做转置其他的步骤与BN相同；如果是RNN的输入矩阵（N * P * t）的话就把P与t放在一起