## 线形神经网络

### 自回归

用归去预测为未来
今天的天气=f(昨天的天气，前天的天气。。。)

### 隐变量自回归
隐变量自回归就是在普通自回归的基础上，加入了一些"看不见但很重要"的因素。
明天客流 = f(昨天客流, 前天客流, ... + 隐藏的天气影响, 隐藏的心理因素, ...)

## 多层感知机

### 权重衰减（L2正则化）
使用L2范数的一个原因是它对权重向量的大分量施加了巨大的惩罚
`Loss=L(w,b)+w^2`

### dropout
在训练过程中，在计算后续层之前向网络的每一层注入噪声。 因为当训练一个有多层的深层网络时，注入噪声只会在输入-输出映射上增强平滑性。

这个想法被称为暂退法（dropout）。 暂退法在前向传播过程中，计算每一内部层的同时注入噪声，这已经成为训练神经网络的常用技术。 这种方法之所以被称为暂退法，因为我们从表面上看是在训练过程中丢弃（drop out）一些神经元。 在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。

![alt text](assets/dropout2.svg)

## 深度学习计算

### 自定义块

1.将输入数据作为其前向传播函数的参数。
2.通过前向传播函数来生成输出。请注意，输出的形状可能与输入的形状不同。例如，我们上面模型中的第一个全连接的层接收一个20维的输入，但是返回一个维度为256的输出。
3.计算其输出关于输入的梯度，可通过其反向传播函数进行访问。通常这是自动发生的。
4.存储和访问前向传播计算所需的参数。
5.根据需要初始化模型参数。

## CNN 卷积神经网络

*卷积神经网络*（convolutional neural network，CNN）是一类强大的、为处理图像数据而设计的神经网络。 基于卷积神经网络架构的模型在计算机视觉领域中已经占主导地位，当今几乎所有的图像识别、目标检测或语义分割相关的学术竞赛和商业应用都以这种方法为基础

### MLP的局限性

1.参数爆炸：28x28 图像展平成 784 输入；若第一层有 1000 隐藏单元，就要 784×1000≈78万参数；更大图像（如 224x224x3=150528）直接失控。

2.忽略空间结构：全连接把像素当前互不相关的独立特征，丢失了图片的空间信息。

3.对平移不鲁棒：同一物体评议，激活模式完全改变。

### CNN是怎么做的

#### 权重共享

在卷积神经网络中，同一个卷积核（滤波器）的参数在输入特征图的不同空间位置上重复使用。也就是说：一组权重 W 不是只服务于某一块区域，而是对整幅图像滑动应用。这种“同一组参数，多处复用”就是权重共享。

**优点**

1. 大幅减少参数量 → 降低过拟合风险
2. 注入“模式可在任意位置出现”的先验 → 获得平移等变性

若用全连接层映射到 100 个隐藏单元：参数 = 32×32×3×100 ≈ 307 万
若用 6 个 5×5 卷积核（stride=1, 不含池化）：参数 = 5×5×3×6 + 6 ≈ 456

#### 局部性

局部性（Locality）指：数据中“相邻”或“近邻”元素之间更相关，远处相关性较弱的规律。

- 图像：猫耳朵特征在 7×7 区域内即可识别，无需看整幅图。
- 语音：音素主要由短时间帧决定。

### 卷积层

![correlation](./assets/correlation.svg)

卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置之后产生输出。 所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。 就像我们之前随机初始化全连接层一样，在训练基于卷积层的模型时，我们也随机初始化卷积核权重。

基于上面定义的`corr2d`函数实现二维卷积层。在`__init__`构造函数中，将`weight`和`bias`声明为两个模型参数。前向传播函数调用`corr2d`函数并添加偏置。

#### 多通道输入

![../_images/conv-multi-in.svg](./assets/conv-multi-in.svg)

#### 多通道输出

在最流行的神经网络架构中，随着神经网络层数的加深，我们常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度。直观地说，我们可以将每个通道看作对不同特征的响应。而现实可能更为复杂一些，因为每个通道不是独立学习的，而是为了共同使用而优化的。因此，多输出通道并不仅是学习多个单通道的检测器。



#### 1*1卷积核

卷积失去了卷积层的特有能力——在高度和宽度维度上，识别相邻元素间相互作用的能力。 其实1*1卷积的唯一计算发生在通道上。

![../_images/conv-1x1.svg](./assets/conv-1x1.svg)

### 汇聚层

通常当我们处理图像时，我们希望逐渐降低隐藏表示的空间分辨率、聚集信息，这样随着我们在神经网络中层叠的上升，每个神经元对其敏感的感受野（输入）就越大。

而我们的机器学习任务通常会跟全局图像的问题有关（例如，“图像是否包含一只猫呢？”），所以我们最后一层的神经元应该对整个输入的全局敏感。通过逐渐聚合信息，生成越来越粗糙的映射，最终实现学习全局表示的目标，同时将卷积图层的所有优势保留在中间层。

*汇聚*（pooling）层，它具有双重目的：降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。

#### 最大汇聚层和平均汇聚层

汇聚层运算符由一个固定形状的窗口组成，该窗口根据其步幅大小在输入的所有区域上滑动，为固定形状窗口（有时称为*汇聚窗口*）遍历的每个位置计算一个输出。 然而，不同于卷积层中的输入与卷积核之间的互相关计算，汇聚层不包含参数。 相反，池运算是确定性的，我们通常计算汇聚窗口中所有元素的最大值或平均值。这些操作分别称为*最大汇聚层*（maximum pooling）和*平均汇聚层*（average pooling）。

![../_images/pooling.svg](./assets/pooling.svg)

### LeNet

总体来看，LeNet（LeNet-5）由两个部分组成：

- 卷积编码器：由两个卷积层组成;

- 全连接层密集块：由三个全连接层组成。![../_images/lenet.svg](./assets/lenet-9051482.svg)

  

  
